{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79036a8c",
   "metadata": {
    "tags": [],
    "id": "79036a8c"
   },
   "source": [
    "# LLM Tracing\n",
    "\n",
    "With the Parea SDK, you can gain visibility into **any LLM application**. Together with the web application, Parea speeds up your debugging, evaluating, and monitoring workflows. Parea is also framework and provider agnostic. Parea traces your prompts and chains, whether deployed from Parea or within your codebase.\n",
    "\n",
    "We will create a simple chat app and instrument logging with Parea. We will also add tags and other metadata to enrich our traces. The chat app uses three 'chained' components to generate a text argument on a provided subject:\n",
    "\n",
    "1. An argument generation function\n",
    "2. Critique function\n",
    "3. Refine function\n",
    "\n",
    "Each function will call an LLM provider; in our case, we'll use OpenAI, but you could quickly call Anthropic or Azure. Parea's dashboard shows how the LLM calls are organized for further analysis and investigation.\n",
    "\n",
    "![DashboardDetailedView](img/dashboard_detailed_view.png)\n",
    "\n",
    "Let's go!\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "First, install the Parea-ai SDK package. If you have an account with Parea, your LLM API Keys will be automatically used, so you won't need to redefine them here.\n",
    "All you need is your Parea API key. Follow the instructions in the [docs](https://docs.parea.ai/api-reference/authentication) to get your api keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5615479e",
   "metadata": {
    "id": "5615479e",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# %pip install -U parea-ai > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ebe2c0",
   "metadata": {
    "id": "06ebe2c0"
   },
   "source": [
    "Next, configure the API Key in the environment to log traces to your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665dbcd",
   "metadata": {
    "id": "f665dbcd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3713f0ab-b90a-4751-ca0a-6d28ceebb385",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PAREA_API_KEY\"] = \"<your-api-key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using the SDK\n",
    "\n",
    "Next, define your chat application. The SDK will automatically generate an inference_id for each of your LLM calls. However, if you want to relate a chain of calls, you can use our helper function to create a trace_id. The function inputs, names, and other information are recorded and visible on Parea's dashboard. This is all done on a background thread to avoid blocking your app's execution.\n",
    "\n",
    "We've created three prompts on Parea and have deployed them. Learn how to deploy a prompt [here](https://docs.parea.ai/deployments/deployments).\n",
    "\n",
    "![Deployed_Prompts](img/deployed_prompts.png)\n",
    "\n",
    "Now we only need the deployment id for each prompt to get started. You can also do this without a deployed prompt; we'll revisit this example in another walkthrough."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f23ca7811eea9dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8c359",
   "metadata": {
    "tags": [],
    "id": "4ba8c359",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from parea.utils.trace_utils import trace\n",
    "from datetime import datetime\n",
    "\n",
    "from parea import Parea\n",
    "from parea.schemas.models import Completion\n",
    "\n",
    "p = Parea(api_key=os.getenv(\"PAREA_API_KEY\"))\n",
    "\n",
    "\n",
    "# We pass the deployment_id and the required inputs to the completion function along with the trace_id\n",
    "@trace\n",
    "def argument_generator(query: str, additional_description: str = \"\") -> str:\n",
    "    return p.completion(\n",
    "        Completion(\n",
    "            deployment_id=\"p-Ar-Oi14-nBxHUiradyql9\",\n",
    "            llm_inputs={\n",
    "                \"additional_description\": additional_description,\n",
    "                \"date\": f\"{datetime.now()}\",\n",
    "                \"query\": query,\n",
    "            },\n",
    "        )\n",
    "    ).content\n",
    "\n",
    "\n",
    "@trace\n",
    "def critic(argument: str) -> str:\n",
    "    return p.completion(\n",
    "        Completion(\n",
    "            deployment_id=\"p-W2yPy93tAczYrxkipjli6\",\n",
    "            llm_inputs={\"argument\": argument},\n",
    "        )\n",
    "    ).content\n",
    "\n",
    "\n",
    "@trace\n",
    "def refiner(query: str, additional_description: str, current_arg: str, criticism: str) -> str:\n",
    "    return p.completion(\n",
    "        Completion(\n",
    "            deployment_id=\"p-8Er1Xo0GDGF2xtpmMOpbn\",\n",
    "            llm_inputs={\n",
    "                \"additional_description\": additional_description,\n",
    "                \"date\": f\"{datetime.now()}\",\n",
    "                \"query\": query,\n",
    "                \"current_arg\": current_arg,\n",
    "                \"criticism\": criticism,\n",
    "            },\n",
    "        )\n",
    "    ).content\n",
    "\n",
    "\n",
    "# This is the parent function which orchestrates the chaining. We'll define our trace_id and trace_name here\n",
    "@trace\n",
    "def argument_chain(query: str, additional_description: str = \"\") -> str:\n",
    "    argument = argument_generator(query, additional_description)\n",
    "    criticism = critic(argument)\n",
    "    return refiner(query, additional_description, argument, criticism)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fb9f73",
   "metadata": {
    "id": "97fb9f73"
   },
   "source": [
    "Now call the chain. If you set up your API key correctly at the start of this notebook, all the results should be traced to [Parea](https://www.optimusprompt.ai/dashboard). We will prompt the app to generate an argument that moonshine is good for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5798a7",
   "metadata": {
    "tags": [],
    "id": "6f5798a7",
    "outputId": "f4c7b093-561f-493d-c7e2-8cde6352f28c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "result = argument_chain(\n",
    "    \"Whether moonshine is good for you.\",\n",
    "    additional_description=\"Provide a concise, few sentence argument on why moonshine is good for you.\",\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad8af8",
   "metadata": {
    "id": "3cad8af8"
   },
   "source": [
    "## Recording feedback\n",
    "\n",
    "The above is all you need to save your app's traces to Parea! You can try changing the functions or raising errors in the above code to see how it's visualized in [Parea](https://www.optimusprompt.ai/dashboard).\n",
    "\n",
    "You can use the trace_id for other things like monitoring user feedback.\n",
    "\n",
    "Below, our `argument_chain2` function is identical to the previous one except that we return the trace_id for use outside the function context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117dca62",
   "metadata": {
    "tags": [],
    "id": "117dca62",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from parea.utils.trace_utils import get_current_trace_id\n",
    "\n",
    "\n",
    "@trace\n",
    "def argument_chain2(query: str, additional_description: str = \"\") -> tuple[str, str]:\n",
    "    trace_id = get_current_trace_id()\n",
    "    argument = argument_generator(query, additional_description)\n",
    "    criticism = critic(argument)\n",
    "    return refiner(query, additional_description, argument, criticism), trace_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b339dd8",
   "metadata": {
    "tags": [],
    "id": "2b339dd8"
   },
   "outputs": [],
   "source": [
    "result, trace_id = argument_chain2(\n",
    "    \"Whether moonshine is good for you.\",\n",
    "    additional_description=\"Provide a concise, few sentence argument on why moonshine is good for you.\",\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c8ea3",
   "metadata": {
    "id": "167c8ea3"
   },
   "source": [
    "With the trace_id, you can now log feedback from a user after the run is completed. Feedback score range from 0.0 (bad) to 1.0 (good)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b378c",
   "metadata": {
    "tags": [],
    "id": "cf7b378c"
   },
   "outputs": [],
   "source": [
    "from parea.schemas.models import FeedbackRequest\n",
    "\n",
    "p.record_feedback(FeedbackRequest(trace_id=trace_id, score=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1656dd86",
   "metadata": {
    "id": "1656dd86"
   },
   "source": [
    "![Feedback](./img/feedback.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc2fad",
   "metadata": {
    "id": "adfc2fad"
   },
   "source": [
    "## Enriching traces\n",
    "\n",
    "One way to make your application traces more useful or actionable is to tag or add metadata to the logs. The completion function accepts additional properties such as:\n",
    "\n",
    "- tags: List[str]\n",
    "- metadata: Dict[str, str] - arbitrary key-value metadata\n",
    "- target: str - a gold standard/expected output\n",
    "- end_user_identifier: str - unique identifier for your end user\n",
    "\n",
    "Below is an example."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from parea.schemas.models import CompletionResponse\n",
    "\n",
    "\n",
    "# let's return the full CompletionResponse to see what other information is returned\n",
    "@trace\n",
    "def refiner2(query: str, additional_description: str, current_arg: str, criticism: str) -> CompletionResponse:\n",
    "    return p.completion(\n",
    "        Completion(\n",
    "            deployment_id=\"p-8Er1Xo0GDGF2xtpmMOpbn\",\n",
    "            llm_inputs={\n",
    "                \"additional_description\": additional_description,\n",
    "                \"date\": f\"{datetime.now()}\",\n",
    "                \"query\": query,\n",
    "                \"current_arg\": current_arg,\n",
    "                \"criticism\": criticism,\n",
    "            },\n",
    "        )\n",
    "    )"
   ],
   "metadata": {
    "id": "cXUHZpZbegIn",
    "ExecuteTime": {
     "end_time": "2023-08-18T03:35:00.724423Z",
     "start_time": "2023-08-18T03:35:00.613460Z"
    }
   },
   "id": "cXUHZpZbegIn",
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac98115b",
   "metadata": {
    "tags": [],
    "id": "ac98115b",
    "ExecuteTime": {
     "end_time": "2023-08-18T03:35:01.978282Z",
     "start_time": "2023-08-18T03:35:01.900898Z"
    }
   },
   "outputs": [],
   "source": [
    "# you can also add metadata and tags via the decorator\n",
    "@trace(\n",
    "    tags=[\"cookbook-example-deployed\", \"feedback_tracked-deployed\"],\n",
    "    metadata={\"source\": \"python-sdk\", \"deployed\": True},\n",
    ")\n",
    "def argument_chain3(query: str, additional_description: str = \"\") -> CompletionResponse:\n",
    "    argument = argument_generator(query, additional_description)\n",
    "    criticism = critic(argument)\n",
    "    return refiner2(query, additional_description, argument, criticism)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import json, attrs\n",
    "\n",
    "result = argument_chain3(\n",
    "    \"Whether moonshine is good for you.\",\n",
    "    additional_description=\"Provide a concise, few sentence argument on why sunshine is good for you.\",\n",
    ")\n",
    "\n",
    "p.record_feedback(\n",
    "    FeedbackRequest(\n",
    "        trace_id=result.trace_id,\n",
    "        score=0.5,\n",
    "        target=\"Moonshine is nice. Full stop.\",\n",
    "    )\n",
    ")\n",
    "print(json.dumps(attrs.asdict(result), indent=4))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggpS9JJ-dn4u",
    "outputId": "6a324d73-8b6c-494d-fad3-f0ba0578cc4a"
   },
   "id": "ggpS9JJ-dn4u",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0b955cab",
   "metadata": {
    "id": "0b955cab"
   },
   "source": [
    "Now you can navigate to the detailed logs with the trace_id to see the additional data.\n",
    "\n",
    "![MetaData](./img/meta_data.png)\n",
    "\n",
    "You can see your logs on the main dashboard and filter, search, and sort by various criteria.\n",
    "\n",
    "![Dashboard](./img/dashboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212bffc-9798-43af-8dd7-d3c5fbf72582",
   "metadata": {
    "id": "d212bffc-9798-43af-8dd7-d3c5fbf72582"
   },
   "source": [
    "## Recap\n",
    "You made an example LLM application in this walkthrough and instrumented it using Parea's SDK.\n",
    "\n",
    "You also added tags and metadata and even logged feedback to the logs. The SDK integrates wonderfully with your deployed prompts on Parea, keeping your code flexible and lightweight. Now you can iterate, debug, and monitor your application with ease.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
